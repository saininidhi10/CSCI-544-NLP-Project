{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT_POS_Tagging.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT-FMNfrEDgY",
        "outputId": "22a2122a-3551-4ce8-805b-a8013604a52a"
      },
      "source": [
        "\"\"\"\n",
        "Installing Pretrained Bert\n",
        "\"\"\"\n",
        "pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.62.3)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.10.0+cu111)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.20.18)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.10.0.2)\n",
            "Requirement already satisfied: botocore<1.24.0,>=1.23.18 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.23.18)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.18->boto3->pytorch_pretrained_bert) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.18->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.18->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz_ISO3UgzVW"
      },
      "source": [
        "#Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPocavSkH9pI"
      },
      "source": [
        "import os\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "import torch.optim as optim\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "import nltk\n",
        "import pdb\n",
        "from pytorch_pretrained_bert import BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sys\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score\n",
        "import csv\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Checking if the machine has the \"GPU\" unit for the computation otherwise selecting the \"CPU\"\n",
        "\"\"\"\n",
        "\n",
        "def get_device():\n",
        "\tdevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\treturn device\n",
        "\n",
        "\"\"\"\n",
        "Getting BERT Tokenizer from the \"bert-based-cased\" mode. \n",
        "This model is pretrained on thousands of Books and Wikipedia Articles.\n",
        "\"\"\"\n",
        "\n",
        "def get_tokenizer():\n",
        "\ttokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "\treturn tokenizer\n",
        "\n",
        "class PosDataset(data.Dataset):\n",
        "    def __init__(self, tagged_sents,tokenizer,tag2idx,idx2tag):\n",
        "        sents, tags_li = [], [] # list of lists\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tag2idx = tag2idx\n",
        "        self.idx2tag = idx2tag\n",
        "        for sent in tagged_sents:\n",
        "            words = [word_pos[0] for word_pos in sent]\n",
        "            tags = [word_pos[1] for word_pos in sent]\n",
        "            sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
        "            tags_li.append([\"<pad>\"] + tags + [\"<pad>\"])\n",
        "        self.sents, self.tags_li = sents, tags_li\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n",
        "\n",
        "        # We give credits only to the first piece.\n",
        "        x, y = [], [] # list of ids\n",
        "        is_heads = [] # list. 1: the token is the first piece of a word\n",
        "        for w, t in zip(words, tags):\n",
        "            tokens = self.tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
        "            xx = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            is_head = [1] + [0]*(len(tokens) - 1)\n",
        "\n",
        "            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n",
        "            yy = [self.tag2idx[each] for each in t]  # (T,)\n",
        "\n",
        "            x.extend(xx)\n",
        "            is_heads.extend(is_head)\n",
        "            y.extend(yy)\n",
        "\n",
        "        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n",
        "\n",
        "        # seqlen\n",
        "        seqlen = len(y)\n",
        "\n",
        "        # to string\n",
        "        words = \" \".join(words)\n",
        "        tags = \" \".join(tags)\n",
        "        return words, x, is_heads, tags, y, seqlen\n",
        "\n",
        "\"\"\"\n",
        "This function is responsible for providing proper padding to the sentences according to the batch size.\n",
        "\"\"\"\n",
        "\n",
        "def pad(batch):\n",
        "    '''Pads to the longest sample'''\n",
        "    f = lambda x: [sample[x] for sample in batch]\n",
        "    words = f(0)\n",
        "    is_heads = f(2)\n",
        "    tags = f(3)\n",
        "    seqlens = f(-1)\n",
        "    maxlen = np.array(seqlens).max()\n",
        "\n",
        "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
        "    x = f(1, maxlen)\n",
        "    y = f(-2, maxlen)\n",
        "\n",
        "\n",
        "    f = torch.LongTensor\n",
        "\n",
        "    return words, f(x), is_heads, tags, f(y), seqlens\n",
        "\n",
        "\"\"\"\n",
        "BERT Layer Architecture\n",
        "This class is Deep Neural Network class in which we are using BERT implementation and \n",
        "adding a Linear layer which is converting the BERT 768 vector output to the size of the tags.\n",
        "\"\"\"\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size=None,device = None):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "        self.fc = nn.Linear(768, vocab_size)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "        \n",
        "        if self.training:\n",
        "            self.bert.train()\n",
        "            encoded_layers, _ = self.bert(x)\n",
        "            enc = encoded_layers[-1]\n",
        "        else:\n",
        "            self.bert.eval()\n",
        "            with torch.no_grad():\n",
        "                encoded_layers, _ = self.bert(x)\n",
        "                enc = encoded_layers[-1]\n",
        "        \n",
        "        logits = self.fc(enc)\n",
        "        y_hat = logits.argmax(-1)\n",
        "        return logits, y, y_hat\n",
        "\n",
        "\"\"\"\n",
        "This function is responsible for the training the extra 1 layer on the top of the pretrained BERT model\n",
        "to fine-tune the BERT model on our dataset.\n",
        "For each epoch, we are using batching to optimize the trainign speed.\n",
        "\"\"\"\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(iterator):\n",
        "        words, x, is_heads, tags, y, seqlens = batch\n",
        "        _y = y # for monitoring\n",
        "        optimizer.zero_grad()\n",
        "        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n",
        "\n",
        "        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
        "        y = y.view(-1)  # (N*T,)\n",
        "\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if i%10==0: # monitoring\n",
        "            print(\"step: {}, loss: {}\".format(i, loss.item()))\n",
        "\n",
        "\"\"\"\n",
        "This function is responsible for evaluating the test results and saving the \n",
        "predictions in results file which can be further used for calculating the \n",
        "Accuracy.\n",
        "At the end, this function also calculated the accuracy on the test dataset\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def eval(model, iterator,tag2idx,idx2tag):\n",
        "    model.eval()\n",
        "\n",
        "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            words, x, is_heads, tags, y, seqlens = batch\n",
        "\n",
        "            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n",
        "\n",
        "            Words.extend(words)\n",
        "            Is_heads.extend(is_heads)\n",
        "            Tags.extend(tags)\n",
        "            Y.extend(y.numpy().tolist())\n",
        "            Y_hat.extend(y_hat.cpu().numpy().tolist())\n",
        "\n",
        "    ## gets results and save\n",
        "    with open(\"result\", 'w') as fout:\n",
        "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
        "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
        "            preds = [idx2tag[hat] for hat in y_hat]\n",
        "            assert len(preds)==len(words.split())==len(tags.split())\n",
        "            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
        "                fout.write(\"{} {} {}\\n\".format(w, t, p))\n",
        "            fout.write(\"\\n\")\n",
        "            \n",
        "    ## calc metric\n",
        "    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n",
        "    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n",
        "\n",
        "    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n",
        "\n",
        "    print(\"acc=%.2f\"%acc)\n",
        "\n",
        "def test(model, iterator,tag2idx,idx2tag):\n",
        "\tmodel.eval()\n",
        "\n",
        "\tWords, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
        "\twith torch.no_grad():\n",
        "\t\tfor i, batch in enumerate(iterator):\n",
        "\t\t\twords, x, is_heads, tags, y, seqlens = batch\n",
        "\n",
        "\t\t_, _, y_hat = model(torch.tensor(x), torch.tensor(y))  # y_hat: (N, T)\n",
        "\n",
        "\t\tWords.extend(words)\n",
        "\t\tIs_heads.extend(is_heads)\n",
        "\t\tTags.extend(tags)\n",
        "\t\tY.extend(y.numpy().tolist())\n",
        "\t\tY_hat.extend(y_hat.cpu().numpy().tolist())\n",
        "\n",
        "\t## get results\n",
        "\tfor words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
        "\t\ty_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
        "\t\tpreds = [idx2tag[hat] for hat in y_hat]\n",
        "\t\tassert len(preds)==len(words.split())==len(tags.split())\n",
        "\t\tret_arr = []\n",
        "\t\tfor w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
        "\t\t\t#print(\"{} {}\".format(w, p))\n",
        "\t\t\tret_arr.append(tuple((w,p)))\n",
        "\treturn ret_arr\n",
        "\t\t\n",
        "            \n",
        "\n",
        "def construct_input(sent):\n",
        "    words = [word_pos for word_pos in sent.split()]\n",
        "    tags = ['-NONE-' for word_pos in sent.split()]\n",
        "    #print(tags)\n",
        "    ret_arr = []\n",
        "    for i,j in zip(words,tags):\n",
        "      ret_arr.append(tuple((i,j)))\n",
        "    return [ret_arr]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txYmQUsxGZFi",
        "outputId": "f002f5f6-1f42-4733-b414-a9185db05fb5"
      },
      "source": [
        "\"\"\"\n",
        "Mounting the Drive\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuZVGnt9cD9_"
      },
      "source": [
        "\"\"\"\n",
        "Function to get train_data from the data file\n",
        "\"\"\"\n",
        "def create_train_data():\n",
        "  data_path = \"/content/drive/MyDrive/NLP_Project/wsj1.train.original\" \n",
        "  #predicted_tags = []\n",
        "  sentence_in=[]\n",
        "  actual_tag=[]\n",
        "  with open(data_path) as f:\n",
        "      sentences = f.readlines()\n",
        "      temp=[]\n",
        "      for sen in sentences:\n",
        "        sentenceSplit = sen.strip(\"\\n\").split(\"\\t\")\n",
        "        if sentenceSplit[0]==\"\":\n",
        "          sentence_in.append(temp)\n",
        "          temp=[]\n",
        "          #print(sentence_in)\n",
        "        else:\n",
        "          temp.append((sentenceSplit[1], sentenceSplit[4]))\n",
        "          actual_tag.append(sentenceSplit[4])\n",
        "  return sentence_in      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FpX45gjhen4"
      },
      "source": [
        "Training and Saving the Model using the Training Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNLibuaD6PT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "882922bd-0c33-4af9-af9d-f736d17f33c6"
      },
      "source": [
        "\"\"\"\n",
        "Training the Model\n",
        "Using Train_iter batch size=8,Eval_iter batch size=8,Adams Optimizer,learning rate=0.000012\n",
        "\n",
        "Evaluating using the built model on test_iter and using tag2idx and idx2tag\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model_dir):\n",
        "\n",
        "\t\n",
        "\ttagged_sents= create_train_data()\n",
        "\tprint(\"tagged are:\",tagged_sents[:10])\n",
        "\ttags = list(set(word_pos[1] for sent in tagged_sents for word_pos in sent))\n",
        "\ttags = [\"<pad>\"] + tags\n",
        "\ttags_str = ','.join(tags)\n",
        "\t# print(len(tags_str))\n",
        "\t# print(tags_str)\n",
        "\ttag2idx = {tag:idx for idx, tag in enumerate(tags)}\n",
        "\tidx2tag = {idx:tag for idx, tag in enumerate(tags)}\n",
        "\ttag2idx[\"-NONE-\"]=len(tag2idx)\n",
        "\t\n",
        "\t\n",
        "\ttrain_data, test_data = train_test_split(tagged_sents, test_size=.1)\n",
        "\t\n",
        "\n",
        "\n",
        "\tdevice = get_device() \n",
        "\ttokenizer = get_tokenizer()\n",
        "\tprint(device)\n",
        "\n",
        "\n",
        "\tmodel = Net(vocab_size=len(tag2idx),device = device)\n",
        "\tmodel.to(device)\n",
        "\tmodel = nn.DataParallel(model)\n",
        "\n",
        "\n",
        "\ttrain_dataset = PosDataset(train_data,tokenizer,tag2idx,idx2tag)\n",
        "\teval_dataset = PosDataset(test_data,tokenizer,tag2idx,idx2tag)\n",
        "\n",
        "\ttrain_iter = data.DataLoader(dataset=train_dataset,\n",
        "                             batch_size=8,\n",
        "                             shuffle=True,\n",
        "                             num_workers=1,\n",
        "                             collate_fn=pad)\n",
        "\ttest_iter = data.DataLoader(dataset=eval_dataset,\n",
        "                             batch_size=8,\n",
        "                             shuffle=False,\n",
        "                             num_workers=1,\n",
        "                             collate_fn=pad)\n",
        "\n",
        "\n",
        "\toptimizer = optim.Adam(model.parameters(), lr = 0.000012)\n",
        "\n",
        "\tcriterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "\ttrain(model, train_iter, optimizer, criterion)\n",
        "\teval(model, test_iter,tag2idx,idx2tag)\n",
        "\n",
        "\n",
        "\tprint(\"Saving model...\")\n",
        "\ttorch.save(model, model_dir + \"/pytorch_model.bin\")\n",
        "\tprint(\"Model saved\")\n",
        "\ttags_arr = [tag2idx,idx2tag]\n",
        "\tprint(\"Pickling tags...\")\n",
        "\tfp = open(model_dir +\"/tags.pkl\",\"wb\")\n",
        "\tpickle.dump(tags_arr,fp)\n",
        "\tfp.close()\n",
        "\tprint(\"Pickling complete...\")\n",
        "\t#print(open('result', 'r').read().splitlines()[:100])\n",
        "\n",
        "\n",
        "if __name__== \"__main__\":\n",
        "\tif (len(sys.argv) < 2):\n",
        "\t\tprint(\"Specify model dir to save\")\n",
        "\telse:\n",
        "\t\ttry:\n",
        "\t\t\tos.mkdir(sys.argv[1])\n",
        "\t\texcept:\n",
        "\t\t\tprint(\"Directory already exists\")\n",
        "\t\ttrain_model(sys.argv[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tagged are: [[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], [('Rudolph', 'NNP'), ('Agnew', 'NNP'), (',', ','), ('55', 'CD'), ('years', 'NNS'), ('old', 'JJ'), ('and', 'CC'), ('former', 'JJ'), ('chairman', 'NN'), ('of', 'IN'), ('Consolidated', 'NNP'), ('Gold', 'NNP'), ('Fields', 'NNP'), ('PLC', 'NNP'), (',', ','), ('was', 'VBD'), ('named', 'VBN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('of', 'IN'), ('this', 'DT'), ('British', 'JJ'), ('industrial', 'JJ'), ('conglomerate', 'NN'), ('.', '.')], [('A', 'DT'), ('form', 'NN'), ('of', 'IN'), ('asbestos', 'NN'), ('once', 'RB'), ('used', 'VBN'), ('to', 'TO'), ('make', 'VB'), ('Kent', 'NNP'), ('cigarette', 'NN'), ('filters', 'NNS'), ('has', 'VBZ'), ('caused', 'VBN'), ('a', 'DT'), ('high', 'JJ'), ('percentage', 'NN'), ('of', 'IN'), ('cancer', 'NN'), ('deaths', 'NNS'), ('among', 'IN'), ('a', 'DT'), ('group', 'NN'), ('of', 'IN'), ('workers', 'NNS'), ('exposed', 'VBN'), ('to', 'TO'), ('it', 'PRP'), ('more', 'RBR'), ('than', 'IN'), ('30', 'CD'), ('years', 'NNS'), ('ago', 'IN'), (',', ','), ('researchers', 'NNS'), ('reported', 'VBD'), ('.', '.')], [('The', 'DT'), ('asbestos', 'NN'), ('fiber', 'NN'), (',', ','), ('crocidolite', 'NN'), (',', ','), ('is', 'VBZ'), ('unusually', 'RB'), ('resilient', 'JJ'), ('once', 'IN'), ('it', 'PRP'), ('enters', 'VBZ'), ('the', 'DT'), ('lungs', 'NNS'), (',', ','), ('with', 'IN'), ('even', 'RB'), ('brief', 'JJ'), ('exposures', 'NNS'), ('to', 'TO'), ('it', 'PRP'), ('causing', 'VBG'), ('symptoms', 'NNS'), ('that', 'WDT'), ('show', 'VBP'), ('up', 'RP'), ('decades', 'NNS'), ('later', 'JJ'), (',', ','), ('researchers', 'NNS'), ('said', 'VBD'), ('.', '.')], [('Lorillard', 'NNP'), ('Inc.', 'NNP'), (',', ','), ('the', 'DT'), ('unit', 'NN'), ('of', 'IN'), ('New', 'JJ'), ('York-based', 'JJ'), ('Loews', 'NNP'), ('Corp.', 'NNP'), ('that', 'WDT'), ('makes', 'VBZ'), ('Kent', 'NNP'), ('cigarettes', 'NNS'), (',', ','), ('stopped', 'VBD'), ('using', 'VBG'), ('crocidolite', 'NN'), ('in', 'IN'), ('its', 'PRP$'), ('Micronite', 'NN'), ('cigarette', 'NN'), ('filters', 'NNS'), ('in', 'IN'), ('1956', 'CD'), ('.', '.')], [('Although', 'IN'), ('preliminary', 'JJ'), ('findings', 'NNS'), ('were', 'VBD'), ('reported', 'VBN'), ('more', 'RBR'), ('than', 'IN'), ('a', 'DT'), ('year', 'NN'), ('ago', 'IN'), (',', ','), ('the', 'DT'), ('latest', 'JJS'), ('results', 'NNS'), ('appear', 'VBP'), ('in', 'IN'), ('today', 'NN'), (\"'s\", 'POS'), ('New', 'NNP'), ('England', 'NNP'), ('Journal', 'NNP'), ('of', 'IN'), ('Medicine', 'NNP'), (',', ','), ('a', 'DT'), ('forum', 'NN'), ('likely', 'JJ'), ('to', 'TO'), ('bring', 'VB'), ('new', 'JJ'), ('attention', 'NN'), ('to', 'TO'), ('the', 'DT'), ('problem', 'NN'), ('.', '.')], [('A', 'DT'), ('Lorillard', 'NNP'), ('spokewoman', 'NN'), ('said', 'VBD'), (',', ','), ('``', '``'), ('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('old', 'JJ'), ('story', 'NN'), ('.', '.')], [('We', 'PRP'), (\"'re\", 'VBP'), ('talking', 'VBG'), ('about', 'IN'), ('years', 'NNS'), ('ago', 'IN'), ('before', 'IN'), ('anyone', 'NN'), ('heard', 'VBD'), ('of', 'IN'), ('asbestos', 'NN'), ('having', 'VBG'), ('any', 'DT'), ('questionable', 'JJ'), ('properties', 'NNS'), ('.', '.')], [('There', 'EX'), ('is', 'VBZ'), ('no', 'DT'), ('asbestos', 'NN'), ('in', 'IN'), ('our', 'PRP$'), ('products', 'NNS'), ('now', 'RB'), ('.', '.'), (\"''\", \"''\")]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 213450/213450 [00:00<00:00, 293339.88B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 404400730/404400730 [00:34<00:00, 11603897.37B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, loss: 3.8522393703460693\n",
            "step: 10, loss: 3.4179584980010986\n",
            "step: 20, loss: 3.025223731994629\n",
            "step: 30, loss: 2.6896438598632812\n",
            "step: 40, loss: 2.2808938026428223\n",
            "step: 50, loss: 2.1795730590820312\n",
            "step: 60, loss: 1.6330621242523193\n",
            "step: 70, loss: 1.4763284921646118\n",
            "step: 80, loss: 1.1412686109542847\n",
            "step: 90, loss: 1.1118792295455933\n",
            "step: 100, loss: 0.9331042766571045\n",
            "step: 110, loss: 0.6677235960960388\n",
            "step: 120, loss: 0.5689499974250793\n",
            "step: 130, loss: 0.5204455256462097\n",
            "step: 140, loss: 0.6345505118370056\n",
            "step: 150, loss: 0.39207538962364197\n",
            "step: 160, loss: 0.5753714442253113\n",
            "step: 170, loss: 0.47636979818344116\n",
            "step: 180, loss: 0.25342056155204773\n",
            "step: 190, loss: 0.2884799540042877\n",
            "step: 200, loss: 0.3245893120765686\n",
            "step: 210, loss: 0.2099679857492447\n",
            "step: 220, loss: 0.26114901900291443\n",
            "step: 230, loss: 0.3720555007457733\n",
            "step: 240, loss: 0.21794283390045166\n",
            "step: 250, loss: 0.1581994891166687\n",
            "step: 260, loss: 0.2596586048603058\n",
            "step: 270, loss: 0.1857927143573761\n",
            "step: 280, loss: 0.2008133977651596\n",
            "step: 290, loss: 0.20189891755580902\n",
            "step: 300, loss: 0.20704226195812225\n",
            "step: 310, loss: 0.267852783203125\n",
            "step: 320, loss: 0.2712749242782593\n",
            "step: 330, loss: 0.18454556167125702\n",
            "step: 340, loss: 0.3139468729496002\n",
            "step: 350, loss: 0.21670620143413544\n",
            "step: 360, loss: 0.19307619333267212\n",
            "step: 370, loss: 0.22380636632442474\n",
            "step: 380, loss: 0.15014423429965973\n",
            "step: 390, loss: 0.18394562602043152\n",
            "step: 400, loss: 0.12530367076396942\n",
            "step: 410, loss: 0.1560903936624527\n",
            "step: 420, loss: 0.16702456772327423\n",
            "step: 430, loss: 0.19954869151115417\n",
            "step: 440, loss: 0.13368543982505798\n",
            "step: 450, loss: 0.14398519694805145\n",
            "step: 460, loss: 0.09626270085573196\n",
            "step: 470, loss: 0.12084376066923141\n",
            "step: 480, loss: 0.14226920902729034\n",
            "step: 490, loss: 0.10808584094047546\n",
            "step: 500, loss: 0.20892709493637085\n",
            "step: 510, loss: 0.1375279724597931\n",
            "step: 520, loss: 0.1120445728302002\n",
            "step: 530, loss: 0.11506800353527069\n",
            "step: 540, loss: 0.11327856779098511\n",
            "step: 550, loss: 0.16871091723442078\n",
            "step: 560, loss: 0.17686496675014496\n",
            "step: 570, loss: 0.20887157320976257\n",
            "step: 580, loss: 0.20624390244483948\n",
            "step: 590, loss: 0.226874440908432\n",
            "step: 600, loss: 0.14030252397060394\n",
            "step: 610, loss: 0.0932362750172615\n",
            "step: 620, loss: 0.1972368210554123\n",
            "step: 630, loss: 0.09308135509490967\n",
            "step: 640, loss: 0.11022999882698059\n",
            "step: 650, loss: 0.14211678504943848\n",
            "step: 660, loss: 0.12775850296020508\n",
            "step: 670, loss: 0.03187898173928261\n",
            "step: 680, loss: 0.07903215289115906\n",
            "step: 690, loss: 0.12991966307163239\n",
            "step: 700, loss: 0.06692424416542053\n",
            "step: 710, loss: 0.07897099107503891\n",
            "step: 720, loss: 0.0864451453089714\n",
            "step: 730, loss: 0.08648856729269028\n",
            "step: 740, loss: 0.08106730878353119\n",
            "step: 750, loss: 0.17529107630252838\n",
            "step: 760, loss: 0.19203944504261017\n",
            "step: 770, loss: 0.040495116263628006\n",
            "step: 780, loss: 0.1632659137248993\n",
            "step: 790, loss: 0.19397559762001038\n",
            "step: 800, loss: 0.0996566116809845\n",
            "step: 810, loss: 0.1605864316225052\n",
            "step: 820, loss: 0.1477864682674408\n",
            "step: 830, loss: 0.08255438506603241\n",
            "step: 840, loss: 0.18230752646923065\n",
            "step: 850, loss: 0.06805593520402908\n",
            "step: 860, loss: 0.12800981104373932\n",
            "step: 870, loss: 0.2512796223163605\n",
            "step: 880, loss: 0.08848331868648529\n",
            "step: 890, loss: 0.09072678536176682\n",
            "step: 900, loss: 0.04350656270980835\n",
            "step: 910, loss: 0.13625586032867432\n",
            "step: 920, loss: 0.13436661660671234\n",
            "step: 930, loss: 0.12920311093330383\n",
            "step: 940, loss: 0.08417287468910217\n",
            "step: 950, loss: 0.16152937710285187\n",
            "step: 960, loss: 0.08134597539901733\n",
            "step: 970, loss: 0.14117732644081116\n",
            "step: 980, loss: 0.20715942978858948\n",
            "step: 990, loss: 0.05202851444482803\n",
            "step: 1000, loss: 0.047354038804769516\n",
            "step: 1010, loss: 0.08199618011713028\n",
            "step: 1020, loss: 0.16767457127571106\n",
            "step: 1030, loss: 0.1837509423494339\n",
            "step: 1040, loss: 0.04320143535733223\n",
            "step: 1050, loss: 0.11844310164451599\n",
            "step: 1060, loss: 0.08427637070417404\n",
            "step: 1070, loss: 0.09673754870891571\n",
            "step: 1080, loss: 0.11793004721403122\n",
            "step: 1090, loss: 0.06775858998298645\n",
            "step: 1100, loss: 0.10961342602968216\n",
            "step: 1110, loss: 0.12870153784751892\n",
            "step: 1120, loss: 0.07217800617218018\n",
            "step: 1130, loss: 0.08422311395406723\n",
            "step: 1140, loss: 0.0768926665186882\n",
            "step: 1150, loss: 0.07542062550783157\n",
            "step: 1160, loss: 0.17913340032100677\n",
            "step: 1170, loss: 0.14142705500125885\n",
            "step: 1180, loss: 0.06519786268472672\n",
            "step: 1190, loss: 0.13960903882980347\n",
            "step: 1200, loss: 0.14637479186058044\n",
            "step: 1210, loss: 0.08058574795722961\n",
            "step: 1220, loss: 0.11162401735782623\n",
            "step: 1230, loss: 0.17568223178386688\n",
            "step: 1240, loss: 0.11674933135509491\n",
            "step: 1250, loss: 0.1300477236509323\n",
            "step: 1260, loss: 0.11607414484024048\n",
            "step: 1270, loss: 0.20304378867149353\n",
            "step: 1280, loss: 0.11133307963609695\n",
            "step: 1290, loss: 0.0661441758275032\n",
            "step: 1300, loss: 0.11082422733306885\n",
            "step: 1310, loss: 0.049501314759254456\n",
            "step: 1320, loss: 0.07297679781913757\n",
            "step: 1330, loss: 0.05136780068278313\n",
            "step: 1340, loss: 0.07020452618598938\n",
            "step: 1350, loss: 0.0680280327796936\n",
            "step: 1360, loss: 0.022533459588885307\n",
            "step: 1370, loss: 0.061145734041929245\n",
            "step: 1380, loss: 0.07225485146045685\n",
            "step: 1390, loss: 0.05502864345908165\n",
            "step: 1400, loss: 0.08624087274074554\n",
            "step: 1410, loss: 0.058179356157779694\n",
            "step: 1420, loss: 0.039727747440338135\n",
            "step: 1430, loss: 0.030951818451285362\n",
            "step: 1440, loss: 0.09672772884368896\n",
            "step: 1450, loss: 0.12122774124145508\n",
            "step: 1460, loss: 0.07472498714923859\n",
            "step: 1470, loss: 0.19138199090957642\n",
            "step: 1480, loss: 0.057978615164756775\n",
            "step: 1490, loss: 0.037248603999614716\n",
            "step: 1500, loss: 0.06176551431417465\n",
            "step: 1510, loss: 0.16864746809005737\n",
            "step: 1520, loss: 0.08063878864049911\n",
            "step: 1530, loss: 0.11189965158700943\n",
            "step: 1540, loss: 0.06714849919080734\n",
            "step: 1550, loss: 0.05676260218024254\n",
            "step: 1560, loss: 0.1690370738506317\n",
            "step: 1570, loss: 0.08896152675151825\n",
            "step: 1580, loss: 0.12221527844667435\n",
            "step: 1590, loss: 0.09763317555189133\n",
            "step: 1600, loss: 0.09655183553695679\n",
            "step: 1610, loss: 0.1639603227376938\n",
            "step: 1620, loss: 0.08946066349744797\n",
            "step: 1630, loss: 0.07929417490959167\n",
            "step: 1640, loss: 0.0984061062335968\n",
            "step: 1650, loss: 0.05584486573934555\n",
            "step: 1660, loss: 0.15073929727077484\n",
            "step: 1670, loss: 0.03477907553315163\n",
            "step: 1680, loss: 0.06780679523944855\n",
            "step: 1690, loss: 0.09072887897491455\n",
            "step: 1700, loss: 0.20654094219207764\n",
            "step: 1710, loss: 0.05247144773602486\n",
            "step: 1720, loss: 0.10802850872278214\n",
            "step: 1730, loss: 0.049520887434482574\n",
            "step: 1740, loss: 0.09398631006479263\n",
            "step: 1750, loss: 0.06268265843391418\n",
            "step: 1760, loss: 0.12652014195919037\n",
            "step: 1770, loss: 0.11942657083272934\n",
            "step: 1780, loss: 0.12427714467048645\n",
            "step: 1790, loss: 0.10095781832933426\n",
            "step: 1800, loss: 0.059582535177469254\n",
            "step: 1810, loss: 0.05159440636634827\n",
            "step: 1820, loss: 0.12645436823368073\n",
            "step: 1830, loss: 0.07551716268062592\n",
            "step: 1840, loss: 0.13710594177246094\n",
            "step: 1850, loss: 0.04263906180858612\n",
            "step: 1860, loss: 0.05277788266539574\n",
            "step: 1870, loss: 0.1905083954334259\n",
            "step: 1880, loss: 0.05435512959957123\n",
            "step: 1890, loss: 0.06206449493765831\n",
            "step: 1900, loss: 0.15539869666099548\n",
            "step: 1910, loss: 0.030986206606030464\n",
            "step: 1920, loss: 0.05482448264956474\n",
            "step: 1930, loss: 0.09866102039813995\n",
            "step: 1940, loss: 0.06591159850358963\n",
            "step: 1950, loss: 0.04607772454619408\n",
            "step: 1960, loss: 0.265873521566391\n",
            "step: 1970, loss: 0.12569916248321533\n",
            "step: 1980, loss: 0.14032171666622162\n",
            "step: 1990, loss: 0.17867356538772583\n",
            "step: 2000, loss: 0.060006700456142426\n",
            "step: 2010, loss: 0.08192091435194016\n",
            "step: 2020, loss: 0.11837390065193176\n",
            "step: 2030, loss: 0.14314647018909454\n",
            "step: 2040, loss: 0.12498821318149567\n",
            "step: 2050, loss: 0.024995483458042145\n",
            "step: 2060, loss: 0.011601069010794163\n",
            "step: 2070, loss: 0.07986826449632645\n",
            "step: 2080, loss: 0.07767166942358017\n",
            "step: 2090, loss: 0.05346998572349548\n",
            "step: 2100, loss: 0.06131165474653244\n",
            "step: 2110, loss: 0.04113632068037987\n",
            "step: 2120, loss: 0.07091522216796875\n",
            "step: 2130, loss: 0.13683678209781647\n",
            "step: 2140, loss: 0.07540758699178696\n",
            "step: 2150, loss: 0.0950201153755188\n",
            "step: 2160, loss: 0.06951915472745895\n",
            "step: 2170, loss: 0.12748776376247406\n",
            "step: 2180, loss: 0.05033663287758827\n",
            "step: 2190, loss: 0.0858164057135582\n",
            "step: 2200, loss: 0.10228117555379868\n",
            "step: 2210, loss: 0.059280361980199814\n",
            "step: 2220, loss: 0.11756709218025208\n",
            "step: 2230, loss: 0.12597624957561493\n",
            "step: 2240, loss: 0.07758314162492752\n",
            "step: 2250, loss: 0.05686931684613228\n",
            "step: 2260, loss: 0.04526727274060249\n",
            "step: 2270, loss: 0.05075467377901077\n",
            "step: 2280, loss: 0.05803023278713226\n",
            "step: 2290, loss: 0.049439869821071625\n",
            "step: 2300, loss: 0.0514727458357811\n",
            "step: 2310, loss: 0.06347334384918213\n",
            "step: 2320, loss: 0.03089381381869316\n",
            "step: 2330, loss: 0.1267983764410019\n",
            "step: 2340, loss: 0.21774379909038544\n",
            "step: 2350, loss: 0.11172650754451752\n",
            "step: 2360, loss: 0.02142247185111046\n",
            "step: 2370, loss: 0.10009343177080154\n",
            "step: 2380, loss: 0.07878763228654861\n",
            "step: 2390, loss: 0.0787971094250679\n",
            "step: 2400, loss: 0.1522115170955658\n",
            "step: 2410, loss: 0.08119883388280869\n",
            "step: 2420, loss: 0.12902572751045227\n",
            "step: 2430, loss: 0.04954773187637329\n",
            "step: 2440, loss: 0.06687634438276291\n",
            "step: 2450, loss: 0.06684216856956482\n",
            "step: 2460, loss: 0.05858419090509415\n",
            "step: 2470, loss: 0.10067737102508545\n",
            "step: 2480, loss: 0.09560433775186539\n",
            "step: 2490, loss: 0.11768288910388947\n",
            "step: 2500, loss: 0.14109206199645996\n",
            "step: 2510, loss: 0.08122381567955017\n",
            "step: 2520, loss: 0.08940327912569046\n",
            "step: 2530, loss: 0.06216863542795181\n",
            "step: 2540, loss: 0.08383354544639587\n",
            "step: 2550, loss: 0.1051928699016571\n",
            "step: 2560, loss: 0.08564577251672745\n",
            "step: 2570, loss: 0.08101467043161392\n",
            "step: 2580, loss: 0.13453179597854614\n",
            "step: 2590, loss: 0.06466794013977051\n",
            "step: 2600, loss: 0.054317597299814224\n",
            "step: 2610, loss: 0.11945793777704239\n",
            "step: 2620, loss: 0.05137951672077179\n",
            "step: 2630, loss: 0.05199320986866951\n",
            "step: 2640, loss: 0.08169623464345932\n",
            "step: 2650, loss: 0.15369708836078644\n",
            "step: 2660, loss: 0.12160763144493103\n",
            "step: 2670, loss: 0.10412793606519699\n",
            "step: 2680, loss: 0.06158217042684555\n",
            "step: 2690, loss: 0.029569271951913834\n",
            "step: 2700, loss: 0.08989903330802917\n",
            "step: 2710, loss: 0.09731322526931763\n",
            "step: 2720, loss: 0.0390097051858902\n",
            "step: 2730, loss: 0.09188438206911087\n",
            "step: 2740, loss: 0.09528151899576187\n",
            "step: 2750, loss: 0.05262548476457596\n",
            "step: 2760, loss: 0.1685042679309845\n",
            "step: 2770, loss: 0.14807988703250885\n",
            "step: 2780, loss: 0.02519870735704899\n",
            "step: 2790, loss: 0.04572660103440285\n",
            "step: 2800, loss: 0.06102781742811203\n",
            "step: 2810, loss: 0.031085696071386337\n",
            "step: 2820, loss: 0.07455429434776306\n",
            "step: 2830, loss: 0.05332076549530029\n",
            "step: 2840, loss: 0.05175076425075531\n",
            "step: 2850, loss: 0.0822378620505333\n",
            "step: 2860, loss: 0.04981914162635803\n",
            "step: 2870, loss: 0.06617908924818039\n",
            "step: 2880, loss: 0.07972009479999542\n",
            "step: 2890, loss: 0.0497751459479332\n",
            "step: 2900, loss: 0.06154187023639679\n",
            "step: 2910, loss: 0.02231563813984394\n",
            "step: 2920, loss: 0.10649336129426956\n",
            "step: 2930, loss: 0.05199878662824631\n",
            "step: 2940, loss: 0.0568440817296505\n",
            "step: 2950, loss: 0.06437219679355621\n",
            "step: 2960, loss: 0.08499876409769058\n",
            "step: 2970, loss: 0.07445629686117172\n",
            "step: 2980, loss: 0.09145200997591019\n",
            "step: 2990, loss: 0.14774486422538757\n",
            "step: 3000, loss: 0.09022241085767746\n",
            "step: 3010, loss: 0.042394913733005524\n",
            "step: 3020, loss: 0.05250842869281769\n",
            "step: 3030, loss: 0.07265447080135345\n",
            "step: 3040, loss: 0.0791289433836937\n",
            "step: 3050, loss: 0.07896950840950012\n",
            "step: 3060, loss: 0.05342899635434151\n",
            "step: 3070, loss: 0.04736366868019104\n",
            "step: 3080, loss: 0.14291654527187347\n",
            "step: 3090, loss: 0.06603439152240753\n",
            "step: 3100, loss: 0.03384112939238548\n",
            "step: 3110, loss: 0.08550508320331573\n",
            "step: 3120, loss: 0.0974377915263176\n",
            "step: 3130, loss: 0.07385393232107162\n",
            "step: 3140, loss: 0.19506597518920898\n",
            "step: 3150, loss: 0.01989763230085373\n",
            "step: 3160, loss: 0.09532200545072556\n",
            "step: 3170, loss: 0.055854786187410355\n",
            "step: 3180, loss: 0.09088565409183502\n",
            "step: 3190, loss: 0.06586857885122299\n",
            "step: 3200, loss: 0.06986720114946365\n",
            "step: 3210, loss: 0.18146061897277832\n",
            "step: 3220, loss: 0.13823407888412476\n",
            "step: 3230, loss: 0.12876151502132416\n",
            "step: 3240, loss: 0.11560679227113724\n",
            "step: 3250, loss: 0.12225116044282913\n",
            "step: 3260, loss: 0.04057060927152634\n",
            "step: 3270, loss: 0.05213146284222603\n",
            "step: 3280, loss: 0.05867000296711922\n",
            "step: 3290, loss: 0.051664482802152634\n",
            "step: 3300, loss: 0.1655571013689041\n",
            "step: 3310, loss: 0.10316159576177597\n",
            "step: 3320, loss: 0.08222033828496933\n",
            "step: 3330, loss: 0.0679730772972107\n",
            "step: 3340, loss: 0.0968846008181572\n",
            "step: 3350, loss: 0.08010821044445038\n",
            "step: 3360, loss: 0.0580999031662941\n",
            "step: 3370, loss: 0.07156766206026077\n",
            "step: 3380, loss: 0.15236400067806244\n",
            "step: 3390, loss: 0.0902593657374382\n",
            "step: 3400, loss: 0.03404036536812782\n",
            "step: 3410, loss: 0.05702401325106621\n",
            "step: 3420, loss: 0.08310294896364212\n",
            "step: 3430, loss: 0.059623315930366516\n",
            "step: 3440, loss: 0.07893514633178711\n",
            "step: 3450, loss: 0.07818864285945892\n",
            "step: 3460, loss: 0.029167048633098602\n",
            "step: 3470, loss: 0.0315907821059227\n",
            "step: 3480, loss: 0.04058771952986717\n",
            "step: 3490, loss: 0.08547645807266235\n",
            "step: 3500, loss: 0.07441936433315277\n",
            "step: 3510, loss: 0.05096093937754631\n",
            "step: 3520, loss: 0.07168830186128616\n",
            "step: 3530, loss: 0.08011990040540695\n",
            "step: 3540, loss: 0.08586081862449646\n",
            "step: 3550, loss: 0.05974781513214111\n",
            "step: 3560, loss: 0.052041471004486084\n",
            "step: 3570, loss: 0.08968400955200195\n",
            "step: 3580, loss: 0.05748150497674942\n",
            "step: 3590, loss: 0.11421384662389755\n",
            "step: 3600, loss: 0.04454939439892769\n",
            "step: 3610, loss: 0.025134410709142685\n",
            "step: 3620, loss: 0.13765199482440948\n",
            "step: 3630, loss: 0.12761162221431732\n",
            "step: 3640, loss: 0.15306943655014038\n",
            "step: 3650, loss: 0.04596342518925667\n",
            "step: 3660, loss: 0.045456890016794205\n",
            "step: 3670, loss: 0.05672898888587952\n",
            "step: 3680, loss: 0.09460484981536865\n",
            "step: 3690, loss: 0.08929944038391113\n",
            "step: 3700, loss: 0.028464144095778465\n",
            "step: 3710, loss: 0.03464598208665848\n",
            "step: 3720, loss: 0.04761209711432457\n",
            "step: 3730, loss: 0.07136077433824539\n",
            "step: 3740, loss: 0.02995956316590309\n",
            "step: 3750, loss: 0.06858005374670029\n",
            "step: 3760, loss: 0.07665068656206131\n",
            "step: 3770, loss: 0.2065354436635971\n",
            "step: 3780, loss: 0.07230109721422195\n",
            "step: 3790, loss: 0.09505848586559296\n",
            "step: 3800, loss: 0.08752583712339401\n",
            "step: 3810, loss: 0.055332280695438385\n",
            "step: 3820, loss: 0.011181643232703209\n",
            "step: 3830, loss: 0.11680331826210022\n",
            "step: 3840, loss: 0.10909688472747803\n",
            "step: 3850, loss: 0.0740387886762619\n",
            "step: 3860, loss: 0.022697588428854942\n",
            "step: 3870, loss: 0.11856932938098907\n",
            "step: 3880, loss: 0.08943597972393036\n",
            "step: 3890, loss: 0.09074404835700989\n",
            "step: 3900, loss: 0.049809254705905914\n",
            "step: 3910, loss: 0.04390757903456688\n",
            "step: 3920, loss: 0.16726790368556976\n",
            "step: 3930, loss: 0.12297512590885162\n",
            "step: 3940, loss: 0.1265348345041275\n",
            "step: 3950, loss: 0.1202150285243988\n",
            "step: 3960, loss: 0.06657086312770844\n",
            "step: 3970, loss: 0.10332068055868149\n",
            "step: 3980, loss: 0.043686896562576294\n",
            "step: 3990, loss: 0.11658410727977753\n",
            "step: 4000, loss: 0.10410559177398682\n",
            "step: 4010, loss: 0.08531591296195984\n",
            "step: 4020, loss: 0.09753381460905075\n",
            "step: 4030, loss: 0.10437026619911194\n",
            "step: 4040, loss: 0.05647527799010277\n",
            "step: 4050, loss: 0.11916455626487732\n",
            "step: 4060, loss: 0.06938548386096954\n",
            "step: 4070, loss: 0.06523853540420532\n",
            "step: 4080, loss: 0.036040954291820526\n",
            "step: 4090, loss: 0.07406969368457794\n",
            "step: 4100, loss: 0.05122898519039154\n",
            "step: 4110, loss: 0.09868156909942627\n",
            "step: 4120, loss: 0.05804992467164993\n",
            "step: 4130, loss: 0.03326673060655594\n",
            "step: 4140, loss: 0.08648505061864853\n",
            "step: 4150, loss: 0.11193347722291946\n",
            "step: 4160, loss: 0.021954437717795372\n",
            "step: 4170, loss: 0.05290884152054787\n",
            "step: 4180, loss: 0.06521891802549362\n",
            "step: 4190, loss: 0.1085701733827591\n",
            "step: 4200, loss: 0.06812013685703278\n",
            "step: 4210, loss: 0.086232028901577\n",
            "step: 4220, loss: 0.06168990954756737\n",
            "step: 4230, loss: 0.04221198335289955\n",
            "step: 4240, loss: 0.05120418593287468\n",
            "step: 4250, loss: 0.1723211705684662\n",
            "step: 4260, loss: 0.0932486355304718\n",
            "step: 4270, loss: 0.07405751943588257\n",
            "step: 4280, loss: 0.049562714993953705\n",
            "step: 4290, loss: 0.09875373542308807\n",
            "acc=0.98\n",
            "Saving model...\n",
            "Model saved\n",
            "Pickling tags...\n",
            "Pickling complete...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEWmGhGx6uyb",
        "outputId": "b1bd4b4e-19f6-484c-e1f2-87c6b04995c8"
      },
      "source": [
        "!pip install bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert\n",
            "  Downloading bert-2.2.0.tar.gz (3.5 kB)\n",
            "Collecting erlastic\n",
            "  Downloading erlastic-2.0.0.tar.gz (6.8 kB)\n",
            "Building wheels for collected packages: bert, erlastic\n",
            "  Building wheel for bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert: filename=bert-2.2.0-py3-none-any.whl size=3766 sha256=9a789f7ca59880eab09c43a69f98b603d0934082e26199bfc1980ee77ed41aa4\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/31/1b/c05f362e347429b7436954d1a2280fe464731e8f569123a848\n",
            "  Building wheel for erlastic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for erlastic: filename=erlastic-2.0.0-py3-none-any.whl size=6795 sha256=462765c7f0f539cc712c545a808e695e437de0970967705eec8a585a892c1dff\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/f1/b4/0b98b1e94775da6a0b1130e342d22af05cd269e1172c19f40f\n",
            "Successfully built bert erlastic\n",
            "Installing collected packages: erlastic, bert\n",
            "Successfully installed bert-2.2.0 erlastic-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oAGB54aEDge"
      },
      "source": [
        "\"\"\"\n",
        "Getting the Dev data from WSJ Penn treebank dataset\n",
        "\"\"\"\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/NLP_Project/wsj1.dev.original\"\n",
        "#predicted_tags = []\n",
        "sentence_in=[]\n",
        "actual_tag=[]\n",
        "with open(data_path) as f:\n",
        "    sentences = f.readlines()\n",
        "    temp=[]\n",
        "    for sen in sentences:\n",
        "      sentenceSplit = sen.strip(\"\\n\").split(\"\\t\")\n",
        "      if sentenceSplit[0]==\"\":\n",
        "        sentence_in.append(temp)\n",
        "        temp=[]\n",
        "        #print(sentence_in)\n",
        "      else:\n",
        "        temp.append(sentenceSplit[1])\n",
        "        actual_tag.append(sentenceSplit[4])\n",
        "\n",
        "res= [' '.join(i) for i in sentence_in]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ykfwwhM6WUs",
        "outputId": "6d5b4623-8990-457b-a2f3-8e0bc3444b99"
      },
      "source": [
        "\"\"\"\n",
        "Predciting the Dev data tags using the loaded model \n",
        "batchsize=8,shuffle=False,num_workers=1,collate_fn=pad\n",
        "\"\"\"\n",
        "\n",
        "predicted_tags=[]\n",
        "def test_model(model_dir):\n",
        "    device = get_device() \n",
        "    tokenizer = get_tokenizer()\n",
        "\n",
        "    print(\"Loading model ...\")\n",
        "    model= torch.load(\"/content/-f/pytorch_model.bin\")\n",
        "    #model = torch.load(model_dir + \"/pytorch_model.bin\")\n",
        "    print(\"Loading model complete\")\n",
        "    print(\"Loading Pickling tags...\")\n",
        "    fp = open(\"/content/-f/tags.pkl\",\"rb\")\n",
        "    tags_arr = pickle.load(fp)\n",
        "    print(\"Loading Pickling tags complete\")\n",
        "    fp.close()\n",
        "    \n",
        "    #while True:\n",
        "    for text in res:\n",
        "  \n",
        "      rt_test_dataset = PosDataset(construct_input(text),tokenizer,tags_arr[0],tags_arr[1])\n",
        "      #rt_test_dataset= res\n",
        "      rt_test_iter = data.DataLoader(dataset=rt_test_dataset,\n",
        "                              batch_size=8,\n",
        "                              shuffle=False,\n",
        "                              num_workers=1,\n",
        "                              collate_fn=pad)\n",
        "\n",
        "\n",
        "      ret_arr = test(model, rt_test_iter,tags_arr[0],tags_arr[1])\n",
        "      predicted_tags.append(ret_arr)\n",
        "      #print(\"ret_arr is:\",ret_arr)\n",
        "if __name__== \"__main__\":\n",
        "\tif (len(sys.argv) < 2):\n",
        "\t\tprint(\"Specify model dir to load model\")\n",
        "\telse:\n",
        "\t\ttest_model(sys.argv[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model ...\n",
            "Loading model complete\n",
            "Loading Pickling tags...\n",
            "Loading Pickling tags complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:180: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlvmpjOdRmX5"
      },
      "source": [
        "res= [[' '.join(i)] for i in sentence_in]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM0zcI3DaYCG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e72d162-cb61-42e4-bda1-65ad2d70fe65"
      },
      "source": [
        "\"\"\"\n",
        "Calculating Accuracy for the Dev dataset using predicted tags and actual tags\n",
        "\"\"\"\n",
        "tags=[]\n",
        "flat_list = [item for sublist in predicted_tags for item in sublist]\n",
        "for res in flat_list:\n",
        "  #print(res)\n",
        "  tags.append(res[1])\n",
        "acc= accuracy_score(actual_tag[:131768], tags)\n",
        "acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9764510351526926"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVLncEPcj2lM"
      },
      "source": [
        "We can observe that we got an accuracy of 97.64% for the Dev data of WSJ Penn Treebank Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12SViAWXkLcX"
      },
      "source": [
        "Getting Test data from WSJ Penn treebank dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjTTvlCIoKPC"
      },
      "source": [
        "data_path = \"/content/drive/MyDrive/NLP_Project/wsj1.test.original\"\n",
        "#predicted_tags = []\n",
        "sentence_in_test=[]\n",
        "actual_tag_test=[]\n",
        "with open(data_path) as f:\n",
        "    sentences_test = f.readlines()\n",
        "    temp_test=[]\n",
        "    for sen in sentences_test:\n",
        "      sentenceSplit_test = sen.strip(\"\\n\").split(\"\\t\")\n",
        "      if sentenceSplit_test[0]==\"\":\n",
        "        sentence_in_test.append(temp_test)\n",
        "        temp_test=[]\n",
        "        #print(sentence_in)\n",
        "      else:\n",
        "        temp_test.append(sentenceSplit_test[1])\n",
        "        actual_tag_test.append(sentenceSplit_test[4])\n",
        "\n",
        "res_test= [' '.join(i) for i in sentence_in_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1lqVfzykqlC"
      },
      "source": [
        "Predicting the Tags for test data using the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01qMU5dRrNfO",
        "outputId": "1bcf77da-6a92-48c6-9c6d-358470d6e737"
      },
      "source": [
        "\n",
        "\n",
        "predicted_tags_test=[]\n",
        "def test_model(model_dir):\n",
        "    device = get_device() \n",
        "    tokenizer = get_tokenizer()\n",
        "\n",
        "    print(\"Loading model ...\")\n",
        "    model= torch.load(\"/content/-f/pytorch_model.bin\")\n",
        "    \n",
        "    print(\"Loading model complete\")\n",
        "    print(\"Loading Pickling tags...\")\n",
        "    fp = open(\"/content/-f/tags.pkl\",\"rb\")\n",
        "    tags_arr = pickle.load(fp)\n",
        "    print(\"Loading Pickling tags complete\")\n",
        "    fp.close()\n",
        "    \n",
        "    #while True:\n",
        "    for text in res_test:\n",
        "  \n",
        "      rt_test_dataset = PosDataset(construct_input(text),tokenizer,tags_arr[0],tags_arr[1])\n",
        "      #rt_test_dataset= res\n",
        "      rt_test_iter = data.DataLoader(dataset=rt_test_dataset,\n",
        "                              batch_size=8,\n",
        "                              shuffle=False,\n",
        "                              num_workers=1,\n",
        "                              collate_fn=pad)\n",
        "\n",
        "\n",
        "      ret_arr_test = test(model, rt_test_iter,tags_arr[0],tags_arr[1])\n",
        "      predicted_tags_test.append(ret_arr_test)\n",
        "      #print(\"ret_arr is:\",ret_arr)\n",
        "if __name__== \"__main__\":\n",
        "\tif (len(sys.argv) < 2):\n",
        "\t\tprint(\"Specify model dir to load model\")\n",
        "\telse:\n",
        "\t\ttest_model(sys.argv[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model ...\n",
            "Loading model complete\n",
            "Loading Pickling tags...\n",
            "Loading Pickling tags complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:180: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLFxIK7iryfT"
      },
      "source": [
        "res_test= [[' '.join(i)] for i in sentence_in_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REaKdcdvlmn5"
      },
      "source": [
        "Calculating the Accuracy score for Test data using actual_tag_test(Actual tags for test data) and tags_test(Predicted tags for test data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW7FAqcbsJ0w",
        "outputId": "3fb24cfb-a219-49df-c68a-6f8af2b656de"
      },
      "source": [
        "\"\"\"\n",
        "Calculating the Accuracy score for Test data using actual_tag_test(Actual tags for test data) and tags_test(Predicted tags for test data)\n",
        "\n",
        "\"\"\"\n",
        "tags_test=[]\n",
        "flat_list_test = [item for sublist in predicted_tags_test for item in sublist]\n",
        "for res in flat_list_test:\n",
        "  #print(res)\n",
        "  tags_test.append(res[1])\n",
        "acc= accuracy_score(actual_tag_test[:129654], tags_test)\n",
        "acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9758048344054175"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSBBt-DAxIGs",
        "outputId": "fc598efe-8573-4034-f264-3695d702c826"
      },
      "source": [
        "len(tags_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "129654"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUqlIpmZI11T",
        "outputId": "fe39ac68-1ef2-4697-ae9b-6b5580990618"
      },
      "source": [
        "\"\"\"\n",
        "Printing the list of predicted tags\n",
        "\"\"\"\n",
        "tags_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['JJ',\n",
              " 'NNS',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'NNPS',\n",
              " 'CC',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'VBD',\n",
              " 'NN',\n",
              " 'WDT',\n",
              " 'MD',\n",
              " 'VB',\n",
              " 'WRB',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'MD',\n",
              " 'VB',\n",
              " 'NN',\n",
              " ',',\n",
              " 'VBG',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'TO',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'POS',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'JJ',\n",
              " 'NNS',\n",
              " '.',\n",
              " 'DT',\n",
              " 'NN',\n",
              " ',',\n",
              " 'WP$',\n",
              " 'NNS',\n",
              " 'VBP',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " '-LRB-',\n",
              " 'NNP',\n",
              " ',',\n",
              " 'NNP',\n",
              " '-RRB-',\n",
              " ',',\n",
              " 'MD',\n",
              " 'VB',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'IN',\n",
              " 'VBG',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'VBG',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'CC',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'WDT',\n",
              " 'MD',\n",
              " 'RB',\n",
              " 'VB',\n",
              " 'VBN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " '.',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'TO',\n",
              " 'NNP',\n",
              " 'NNS',\n",
              " 'RB',\n",
              " ',',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'JJ',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " '.',\n",
              " '``',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " '``',\n",
              " 'JJ',\n",
              " \"''\",\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'JJ',\n",
              " 'CC',\n",
              " 'JJ',\n",
              " ',',\n",
              " 'RB',\n",
              " 'RBR',\n",
              " 'JJ',\n",
              " 'IN',\n",
              " 'JJ',\n",
              " 'NNP',\n",
              " 'NN',\n",
              " ',',\n",
              " \"''\",\n",
              " 'VBD',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " '-LRB-',\n",
              " 'NNP',\n",
              " ',',\n",
              " 'NNP',\n",
              " '-RRB-',\n",
              " ',',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'POS',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " '.',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'VBG',\n",
              " '$',\n",
              " 'CD',\n",
              " 'CD',\n",
              " 'IN',\n",
              " 'NN',\n",
              " 'VBN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'RB',\n",
              " 'VBN',\n",
              " 'NNP',\n",
              " '.',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'VBD',\n",
              " 'VBN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'NN',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'VBG',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'NN',\n",
              " '.',\n",
              " 'DT',\n",
              " '$',\n",
              " 'CD',\n",
              " 'CD',\n",
              " 'MD',\n",
              " 'VB',\n",
              " 'VBN',\n",
              " 'IN',\n",
              " 'NNP',\n",
              " 'NNS',\n",
              " ',',\n",
              " 'WDT',\n",
              " 'VBP',\n",
              " 'JJR',\n",
              " 'NN',\n",
              " 'NNS',\n",
              " '.',\n",
              " 'CC',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'RB',\n",
              " 'VBZ',\n",
              " '``',\n",
              " 'NN',\n",
              " \"''\",\n",
              " 'NN',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NNS',\n",
              " 'IN',\n",
              " 'NNS',\n",
              " 'WDT',\n",
              " 'VBP',\n",
              " 'VBN',\n",
              " ',',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNS',\n",
              " 'MD',\n",
              " 'VB',\n",
              " 'VBN',\n",
              " 'RB',\n",
              " '.',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'MD',\n",
              " 'VB',\n",
              " 'VBN',\n",
              " 'RP',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNS',\n",
              " 'VBP',\n",
              " 'VBN',\n",
              " ',',\n",
              " 'VBG',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " '$',\n",
              " 'CD',\n",
              " 'CD',\n",
              " ',',\n",
              " 'CC',\n",
              " '$',\n",
              " 'CD',\n",
              " 'CD',\n",
              " 'VBG',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'CD',\n",
              " 'NNS',\n",
              " '.',\n",
              " '``',\n",
              " 'PRP',\n",
              " 'VBZ',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'WDT',\n",
              " 'RB',\n",
              " 'VBZ',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'VBN',\n",
              " ',',\n",
              " \"''\",\n",
              " 'VBD',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " ',',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " '.',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'RB',\n",
              " 'VBN',\n",
              " 'RB',\n",
              " '$',\n",
              " 'CD',\n",
              " 'CD',\n",
              " 'VBG',\n",
              " 'CD',\n",
              " 'JJ',\n",
              " 'NNS',\n",
              " ',',\n",
              " 'CC',\n",
              " 'PRP',\n",
              " 'VBZ',\n",
              " 'JJ',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'CC',\n",
              " 'VB',\n",
              " 'CD',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " '.',\n",
              " 'JJ',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'NN',\n",
              " ',',\n",
              " 'PRP',\n",
              " 'VBD',\n",
              " ',',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'MD',\n",
              " 'VB',\n",
              " 'VBN',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'NNS',\n",
              " 'IN',\n",
              " 'NN',\n",
              " 'MD',\n",
              " 'VB',\n",
              " 'VBN',\n",
              " 'IN',\n",
              " 'VBG',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NNS',\n",
              " '.',\n",
              " '``',\n",
              " 'PRP',\n",
              " 'MD',\n",
              " 'VB',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'IN',\n",
              " 'PRP',\n",
              " 'VBP',\n",
              " 'VBN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNS',\n",
              " 'IN',\n",
              " 'PRP',\n",
              " 'MD',\n",
              " 'VB',\n",
              " 'RB',\n",
              " ',',\n",
              " \"''\",\n",
              " 'PRP',\n",
              " 'VBD',\n",
              " '.',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'VBN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " '.',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'VB',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'IN',\n",
              " 'TO',\n",
              " '$',\n",
              " 'CD',\n",
              " 'CD',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " '.',\n",
              " 'RB',\n",
              " ',',\n",
              " 'PRP',\n",
              " 'VBZ',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'POS',\n",
              " 'JJ',\n",
              " 'NNS',\n",
              " 'MD',\n",
              " 'RB',\n",
              " 'VB',\n",
              " '$',\n",
              " 'CD',\n",
              " 'CD',\n",
              " ',',\n",
              " 'CC',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'VBN',\n",
              " 'IN',\n",
              " 'VBG',\n",
              " 'NNS',\n",
              " 'CC',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " ',',\n",
              " 'CC',\n",
              " 'VBG',\n",
              " 'IN',\n",
              " 'PRP',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNS',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'VBZ',\n",
              " '.',\n",
              " 'CC',\n",
              " 'NNP',\n",
              " 'VBD',\n",
              " 'RB',\n",
              " 'VB',\n",
              " 'CC',\n",
              " 'VB',\n",
              " 'JJR',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " ',',\n",
              " 'VBP',\n",
              " 'NNS',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'POS',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " ',',\n",
              " 'CC',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " '-LRB-',\n",
              " 'NNP',\n",
              " ',',\n",
              " 'NNP',\n",
              " '.',\n",
              " '-RRB-',\n",
              " 'VBD',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'VBZ',\n",
              " 'VBN',\n",
              " 'JJ',\n",
              " 'IN',\n",
              " 'RB',\n",
              " 'VBG',\n",
              " 'NNP',\n",
              " 'JJ',\n",
              " '.',\n",
              " '``',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'TO',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'CD',\n",
              " 'IN',\n",
              " 'NNPS',\n",
              " 'CC',\n",
              " 'NNPS',\n",
              " ',',\n",
              " 'WDT',\n",
              " 'VBZ',\n",
              " 'TO',\n",
              " 'PRP',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'JJ',\n",
              " ',',\n",
              " \"''\",\n",
              " 'PRP',\n",
              " 'VBD',\n",
              " '.',\n",
              " '``',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'VBZ',\n",
              " 'VBG',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'IN',\n",
              " 'PRP',\n",
              " 'VBP',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'NN',\n",
              " '.',\n",
              " \"''\",\n",
              " 'DT',\n",
              " 'NNPS',\n",
              " 'CC',\n",
              " 'NNPS',\n",
              " 'NNP',\n",
              " 'MD',\n",
              " 'VB',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'NNP',\n",
              " '.',\n",
              " 'PRP',\n",
              " 'VBP',\n",
              " 'IN',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'IN',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " '.',\n",
              " 'RB',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNS',\n",
              " 'IN',\n",
              " 'NNP',\n",
              " 'POS',\n",
              " 'JJ',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'CC',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'WDT',\n",
              " 'VBZ',\n",
              " 'VBN',\n",
              " ',',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'NNS',\n",
              " 'VBP',\n",
              " 'VBG',\n",
              " 'RP',\n",
              " 'JJ',\n",
              " 'NNS',\n",
              " 'VBG',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " ':',\n",
              " 'VB',\n",
              " 'RP',\n",
              " 'VBG',\n",
              " ',',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'RB',\n",
              " 'JJ',\n",
              " '.',\n",
              " 'PRP$',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'NNS',\n",
              " 'IN',\n",
              " 'VBG',\n",
              " 'DT',\n",
              " 'NN',\n",
              " ',',\n",
              " 'IN',\n",
              " 'JJ',\n",
              " 'NNS',\n",
              " 'VBD',\n",
              " 'IN',\n",
              " 'NNS',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'NNP',\n",
              " 'RB',\n",
              " 'NNS',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'CD',\n",
              " 'NN',\n",
              " ',',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'NNS',\n",
              " 'VBD',\n",
              " 'RP',\n",
              " 'NNS',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'NNS',\n",
              " '.',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'RB',\n",
              " ',',\n",
              " 'PRP',\n",
              " 'VBP',\n",
              " 'VBG',\n",
              " 'RB',\n",
              " 'RBR',\n",
              " '.',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'VBD',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'CD',\n",
              " 'NN',\n",
              " 'NNP',\n",
              " 'NN',\n",
              " 'CC',\n",
              " 'VBD',\n",
              " 'PRP',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " '.',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'VBD',\n",
              " 'JJ',\n",
              " 'NNS',\n",
              " 'IN',\n",
              " 'NNS',\n",
              " 'NN',\n",
              " ',',\n",
              " 'CC',\n",
              " 'VBD',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'VBG',\n",
              " 'NN',\n",
              " '.',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'IN',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'VBD',\n",
              " 'RB',\n",
              " 'VBN',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'NNS',\n",
              " '.',\n",
              " 'PRP',\n",
              " 'VBD',\n",
              " 'VBG',\n",
              " 'PRP',\n",
              " 'IN',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'POS',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'NN',\n",
              " 'CC',\n",
              " 'VBD',\n",
              " 'RB',\n",
              " 'TO',\n",
              " 'WRB',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBD',\n",
              " 'NN',\n",
              " '.',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'NNS',\n",
              " ',',\n",
              " 'VBG',\n",
              " 'NNP',\n",
              " 'NNP',\n",
              " 'CC',\n",
              " 'NNP',\n",
              " ',',\n",
              " 'VBD',\n",
              " 'VBG',\n",
              " 'RP',\n",
              " 'JJ',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'NNS',\n",
              " '.',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'NNS',\n",
              " 'VBD',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'RB',\n",
              " ',',\n",
              " 'WRB',\n",
              " 'JJ',\n",
              " 'NNS',\n",
              " 'VBD',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'NNS',\n",
              " 'CC',\n",
              " 'VBD',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " '.',\n",
              " 'DT',\n",
              " 'NN',\n",
              " ',',\n",
              " 'DT',\n",
              " 'NNS',\n",
              " 'VBD',\n",
              " 'JJ',\n",
              " '.',\n",
              " 'NNP',\n",
              " ',',\n",
              " 'IN',\n",
              " 'NN',\n",
              " ',',\n",
              " 'VBD',\n",
              " 'NNS',\n",
              " 'JJ',\n",
              " 'NNS',\n",
              " 'RB',\n",
              " 'IN',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'NN',\n",
              " '.',\n",
              " 'WRB',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBD',\n",
              " 'IN',\n",
              " 'PRP$',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'NNP',\n",
              " 'NN',\n",
              " ',',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'VBD',\n",
              " 'JJ',\n",
              " 'NNS',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'NNS',\n",
              " 'IN',\n",
              " 'PDT',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'NNS',\n",
              " '.',\n",
              " 'DT',\n",
              " 'NNS',\n",
              " 'VBD',\n",
              " 'NNP',\n",
              " 'POS',\n",
              " 'JJ',\n",
              " 'JJ',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " ',',\n",
              " '``',\n",
              " 'NNP',\n",
              " 'VBZ',\n",
              " 'JJ',\n",
              " 'IN',\n",
              " 'PRP$',\n",
              " 'NN',\n",
              " '.',\n",
              " \"''\",\n",
              " 'DT',\n",
              " 'NNP',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " ',',\n",
              " 'WDT',\n",
              " 'RB',\n",
              " 'VBD',\n",
              " 'VBG',\n",
              " 'CC',\n",
              " 'WDT',\n",
              " 'JJ',\n",
              " 'NNS',\n",
              " 'VBD',\n",
              " 'RB',\n",
              " 'VB',\n",
              " 'IN',\n",
              " ',',\n",
              " 'VBD',\n",
              " 'IN',\n",
              " 'JJ',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'NNS',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " '.',\n",
              " '``',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'NN',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'IN',\n",
              " 'PRP',\n",
              " 'MD',\n",
              " 'VB',\n",
              " 'TO',\n",
              " 'PRP',\n",
              " ',',\n",
              " \"''\",\n",
              " 'PRP',\n",
              " 'VBZ',\n",
              " '.',\n",
              " '``',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'RB',\n",
              " 'JJ',\n",
              " '.',\n",
              " 'PRP',\n",
              " 'MD',\n",
              " 'VB',\n",
              " 'VBN',\n",
              " 'RB',\n",
              " 'JJ',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'IN',\n",
              " 'IN',\n",
              " 'NNP',\n",
              " '.',\n",
              " 'PRP',\n",
              " 'VBD',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'IN',\n",
              " 'PRP',\n",
              " 'RB',\n",
              " 'IN',\n",
              " 'NN',\n",
              " '.',\n",
              " \"''\",\n",
              " 'NN',\n",
              " 'POS',\n",
              " 'NNP',\n",
              " 'NN',\n",
              " 'VBZ',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'RBR',\n",
              " ',',\n",
              " 'VBG',\n",
              " 'NNS',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'CC',\n",
              " 'RB',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'RP',\n",
              " 'IN',\n",
              " 'NNP',\n",
              " '.',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " '``',\n",
              " 'NN',\n",
              " ',',\n",
              " \"''\",\n",
              " 'PRP',\n",
              " 'VBZ',\n",
              " ',',\n",
              " '``',\n",
              " 'VBN',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NNS',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'JJ',\n",
              " 'NN',\n",
              " ',',\n",
              " 'DT',\n",
              " 'NNS',\n",
              " 'VBP',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'PRP$',\n",
              " 'NNS',\n",
              " 'VBP',\n",
              " 'VBN',\n",
              " 'TO',\n",
              " 'VB',\n",
              " 'VB',\n",
              " 'PRP',\n",
              " 'IN',\n",
              " 'DT',\n",
              " 'NN',\n",
              " 'POS',\n",
              " 'NN',\n",
              " '.',\n",
              " \"''\",\n",
              " 'PRP',\n",
              " 'VBZ',\n",
              " 'RP',\n",
              " 'TO',\n",
              " 'VB',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlvZmjZDG774",
        "outputId": "476d12fa-c74c-4f1c-9ac0-c287c63a0871"
      },
      "source": [
        "\"\"\"\n",
        "Calculating Accuracy for the test data using tags_test(predicted_tags) list and actual_tag_test(actual tags)\n",
        "\"\"\"\n",
        "count=0\n",
        "for i in range(len(tags_test)):\n",
        "  if tags_test[i]==actual_tag_test[i]:\n",
        "    count+=1\n",
        "check=count/len(tags_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9758048344054175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHirSv69KZzb"
      },
      "source": [
        "\"\"\"\n",
        "Converting test data of WSJ Penn tree bank  into CSV\n",
        "\"\"\"\n",
        "test_2 = csv.reader(open('/content/drive/MyDrive/NLP_Project/wsj1.test.original') , delimiter=\"\\t\")\n",
        "data_test_2=list(test_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Iwk1ye6KyYb",
        "outputId": "d27d8613-6519-45c2-adcd-7a12156c59ef"
      },
      "source": [
        "data_test_2[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1', 'Influential', '_', 'JJ', 'JJ', '_', '2', 'NMOD']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbD4BzbxR2Sq",
        "outputId": "e8966886-ef9c-42c4-fc5a-1ec21f961ed5"
      },
      "source": [
        "len(data_test_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "135117"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRx3an9-TEFs"
      },
      "source": [
        "\"\"\"\n",
        "Removing empty lists from the data_test\n",
        "\"\"\"\n",
        "res = [ele for ele in data_test_2 if ele != []]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vV6Zuh0Tv0X",
        "outputId": "02db4ac7-9d03-411f-f7a9-54f8051e0a88"
      },
      "source": [
        "len(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "129654"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb5Nl2xLSLqW",
        "outputId": "9eceed5f-8c9e-47f4-ba3a-d28eec517dcc"
      },
      "source": [
        "len(tags_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "129654"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7-ANFqr8x37"
      },
      "source": [
        "Appending Predicted tags to the Data list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVgDc4FuSXIL"
      },
      "source": [
        "for i in range(len(res)):\n",
        "  res[i].append(tags_test[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsRhdUrbmdCB"
      },
      "source": [
        "Getting the Predictions for Test data to evaluate the performance of the Model Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fho7Z3Kg3uB_"
      },
      "source": [
        "\"\"\"\n",
        "Getting the predicted output file for test file \n",
        "\"\"\"\n",
        "pred = open('/content/drive/MyDrive/NLP_Project/pred.out', 'w')\n",
        "\n",
        "for i in range(len(res)):\n",
        "\n",
        "\n",
        "\n",
        "   pred.write(str(res[i][0])  + '\\t' + str(res[i][1]) + '\\t' +str(res[i][8])+'\\n')\n",
        "   \n",
        "pred.close()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_uuT0YbUSAb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}