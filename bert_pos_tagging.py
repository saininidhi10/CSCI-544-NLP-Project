# -*- coding: utf-8 -*-
"""BERT_POS_Tagging.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KFlUDXTPio4yl7O2P_ZEVUnPeMSsrAQ1
"""

"""
Installing Pretrained Bert
"""
pip install pytorch_pretrained_bert

"""#Importing Required Libraries"""

import os
from tqdm import tqdm_notebook as tqdm
import numpy as np
import torch
import torch.nn as nn
from torch.utils import data
import torch.optim as optim
from pytorch_pretrained_bert import BertTokenizer
import nltk
import pdb
from pytorch_pretrained_bert import BertModel
from sklearn.model_selection import train_test_split
import sys
import pickle
from sklearn.metrics import accuracy_score
import csv



"""
Checking if the machine has the "GPU" unit for the computation otherwise selecting the "CPU"
"""

def get_device():
	device = 'cuda' if torch.cuda.is_available() else 'cpu'
	return device

"""
Getting BERT Tokenizer from the "bert-based-cased" mode. 
This model is pretrained on thousands of Books and Wikipedia Articles.
"""

def get_tokenizer():
	tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)
	return tokenizer

class PosDataset(data.Dataset):
    def __init__(self, tagged_sents,tokenizer,tag2idx,idx2tag):
        sents, tags_li = [], [] # list of lists
        self.tokenizer = tokenizer
        self.tag2idx = tag2idx
        self.idx2tag = idx2tag
        for sent in tagged_sents:
            words = [word_pos[0] for word_pos in sent]
            tags = [word_pos[1] for word_pos in sent]
            sents.append(["[CLS]"] + words + ["[SEP]"])
            tags_li.append(["<pad>"] + tags + ["<pad>"])
        self.sents, self.tags_li = sents, tags_li

    def __len__(self):
        return len(self.sents)

    def __getitem__(self, idx):
        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list

        # We give credits only to the first piece.
        x, y = [], [] # list of ids
        is_heads = [] # list. 1: the token is the first piece of a word
        for w, t in zip(words, tags):
            tokens = self.tokenizer.tokenize(w) if w not in ("[CLS]", "[SEP]") else [w]
            xx = self.tokenizer.convert_tokens_to_ids(tokens)

            is_head = [1] + [0]*(len(tokens) - 1)

            t = [t] + ["<pad>"] * (len(tokens) - 1)  # <PAD>: no decision
            yy = [self.tag2idx[each] for each in t]  # (T,)

            x.extend(xx)
            is_heads.extend(is_head)
            y.extend(yy)

        assert len(x)==len(y)==len(is_heads), "len(x)={}, len(y)={}, len(is_heads)={}".format(len(x), len(y), len(is_heads))

        # seqlen
        seqlen = len(y)

        # to string
        words = " ".join(words)
        tags = " ".join(tags)
        return words, x, is_heads, tags, y, seqlen

"""
This function is responsible for providing proper padding to the sentences according to the batch size.
"""

def pad(batch):
    '''Pads to the longest sample'''
    f = lambda x: [sample[x] for sample in batch]
    words = f(0)
    is_heads = f(2)
    tags = f(3)
    seqlens = f(-1)
    maxlen = np.array(seqlens).max()

    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>
    x = f(1, maxlen)
    y = f(-2, maxlen)


    f = torch.LongTensor

    return words, f(x), is_heads, tags, f(y), seqlens

"""
BERT Layer Architecture
This class is Deep Neural Network class in which we are using BERT implementation and 
adding a Linear layer which is converting the BERT 768 vector output to the size of the tags.
"""

class Net(nn.Module):
    def __init__(self, vocab_size=None,device = None):
        super().__init__()
        self.bert = BertModel.from_pretrained('bert-base-cased')

        self.fc = nn.Linear(768, vocab_size)
        self.device = device

    def forward(self, x, y):
        x = x.to(self.device)
        y = y.to(self.device)
        
        if self.training:
            self.bert.train()
            encoded_layers, _ = self.bert(x)
            enc = encoded_layers[-1]
        else:
            self.bert.eval()
            with torch.no_grad():
                encoded_layers, _ = self.bert(x)
                enc = encoded_layers[-1]
        
        logits = self.fc(enc)
        y_hat = logits.argmax(-1)
        return logits, y, y_hat

"""
This function is responsible for the training the extra 1 layer on the top of the pretrained BERT model
to fine-tune the BERT model on our dataset.
For each epoch, we are using batching to optimize the trainign speed.
"""

def train(model, iterator, optimizer, criterion):
    model.train()
    for i, batch in enumerate(iterator):
        words, x, is_heads, tags, y, seqlens = batch
        _y = y # for monitoring
        optimizer.zero_grad()
        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)

        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)
        y = y.view(-1)  # (N*T,)

        loss = criterion(logits, y)
        loss.backward()

        optimizer.step()

        if i%10==0: # monitoring
            print("step: {}, loss: {}".format(i, loss.item()))

"""
This function is responsible for evaluating the test results and saving the 
predictions in results file which can be further used for calculating the 
Accuracy.
At the end, this function also calculated the accuracy on the test dataset
"""


def eval(model, iterator,tag2idx,idx2tag):
    model.eval()

    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []
    with torch.no_grad():
        for i, batch in enumerate(iterator):
            words, x, is_heads, tags, y, seqlens = batch

            _, _, y_hat = model(x, y)  # y_hat: (N, T)

            Words.extend(words)
            Is_heads.extend(is_heads)
            Tags.extend(tags)
            Y.extend(y.numpy().tolist())
            Y_hat.extend(y_hat.cpu().numpy().tolist())

    ## gets results and save
    with open("result", 'w') as fout:
        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):
            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]
            preds = [idx2tag[hat] for hat in y_hat]
            assert len(preds)==len(words.split())==len(tags.split())
            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):
                fout.write("{} {} {}\n".format(w, t, p))
            fout.write("\n")
            
    ## calc metric
    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])
    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])

    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)

    print("acc=%.2f"%acc)

def test(model, iterator,tag2idx,idx2tag):
	model.eval()

	Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []
	with torch.no_grad():
		for i, batch in enumerate(iterator):
			words, x, is_heads, tags, y, seqlens = batch

		_, _, y_hat = model(torch.tensor(x), torch.tensor(y))  # y_hat: (N, T)

		Words.extend(words)
		Is_heads.extend(is_heads)
		Tags.extend(tags)
		Y.extend(y.numpy().tolist())
		Y_hat.extend(y_hat.cpu().numpy().tolist())

	## get results
	for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):
		y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]
		preds = [idx2tag[hat] for hat in y_hat]
		assert len(preds)==len(words.split())==len(tags.split())
		ret_arr = []
		for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):
			#print("{} {}".format(w, p))
			ret_arr.append(tuple((w,p)))
	return ret_arr
		
            

def construct_input(sent):
    words = [word_pos for word_pos in sent.split()]
    tags = ['-NONE-' for word_pos in sent.split()]
    #print(tags)
    ret_arr = []
    for i,j in zip(words,tags):
      ret_arr.append(tuple((i,j)))
    return [ret_arr]

"""
Mounting the Drive
"""
from google.colab import drive
drive.mount('/content/drive')

"""
Function to get train_data from the data file
"""
def create_train_data():
  data_path = "/content/drive/MyDrive/NLP_Project/wsj1.train.original" 
  #predicted_tags = []
  sentence_in=[]
  actual_tag=[]
  with open(data_path) as f:
      sentences = f.readlines()
      temp=[]
      for sen in sentences:
        sentenceSplit = sen.strip("\n").split("\t")
        if sentenceSplit[0]=="":
          sentence_in.append(temp)
          temp=[]
          #print(sentence_in)
        else:
          temp.append((sentenceSplit[1], sentenceSplit[4]))
          actual_tag.append(sentenceSplit[4])
  return sentence_in

"""Training and Saving the Model using the Training Dataset

"""

"""
Training the Model
Using Train_iter batch size=8,Eval_iter batch size=8,Adams Optimizer,learning rate=0.000012

Evaluating using the built model on test_iter and using tag2idx and idx2tag
"""



def train_model(model_dir):

	
	tagged_sents= create_train_data()
	print("tagged are:",tagged_sents[:10])
	tags = list(set(word_pos[1] for sent in tagged_sents for word_pos in sent))
	tags = ["<pad>"] + tags
	tags_str = ','.join(tags)
	# print(len(tags_str))
	# print(tags_str)
	tag2idx = {tag:idx for idx, tag in enumerate(tags)}
	idx2tag = {idx:tag for idx, tag in enumerate(tags)}
	tag2idx["-NONE-"]=len(tag2idx)
	
	
	train_data, test_data = train_test_split(tagged_sents, test_size=.1)
	


	device = get_device() 
	tokenizer = get_tokenizer()
	print(device)


	model = Net(vocab_size=len(tag2idx),device = device)
	model.to(device)
	model = nn.DataParallel(model)


	train_dataset = PosDataset(train_data,tokenizer,tag2idx,idx2tag)
	eval_dataset = PosDataset(test_data,tokenizer,tag2idx,idx2tag)

	train_iter = data.DataLoader(dataset=train_dataset,
                             batch_size=8,
                             shuffle=True,
                             num_workers=1,
                             collate_fn=pad)
	test_iter = data.DataLoader(dataset=eval_dataset,
                             batch_size=8,
                             shuffle=False,
                             num_workers=1,
                             collate_fn=pad)


	optimizer = optim.Adam(model.parameters(), lr = 0.000012)

	criterion = nn.CrossEntropyLoss(ignore_index=0)

	train(model, train_iter, optimizer, criterion)
	eval(model, test_iter,tag2idx,idx2tag)


	print("Saving model...")
	torch.save(model, model_dir + "/pytorch_model.bin")
	print("Model saved")
	tags_arr = [tag2idx,idx2tag]
	print("Pickling tags...")
	fp = open(model_dir +"/tags.pkl","wb")
	pickle.dump(tags_arr,fp)
	fp.close()
	print("Pickling complete...")
	#print(open('result', 'r').read().splitlines()[:100])


if __name__== "__main__":
	if (len(sys.argv) < 2):
		print("Specify model dir to save")
	else:
		try:
			os.mkdir(sys.argv[1])
		except:
			print("Directory already exists")
		train_model(sys.argv[1])

!pip install bert

"""
Getting the Dev data from WSJ Penn treebank dataset
"""

data_path = "/content/drive/MyDrive/NLP_Project/wsj1.dev.original"
#predicted_tags = []
sentence_in=[]
actual_tag=[]
with open(data_path) as f:
    sentences = f.readlines()
    temp=[]
    for sen in sentences:
      sentenceSplit = sen.strip("\n").split("\t")
      if sentenceSplit[0]=="":
        sentence_in.append(temp)
        temp=[]
        #print(sentence_in)
      else:
        temp.append(sentenceSplit[1])
        actual_tag.append(sentenceSplit[4])

res= [' '.join(i) for i in sentence_in]

"""
Predciting the Dev data tags using the loaded model 
batchsize=8,shuffle=False,num_workers=1,collate_fn=pad
"""

predicted_tags=[]
def test_model(model_dir):
    device = get_device() 
    tokenizer = get_tokenizer()

    print("Loading model ...")
    model= torch.load("/content/-f/pytorch_model.bin")
    #model = torch.load(model_dir + "/pytorch_model.bin")
    print("Loading model complete")
    print("Loading Pickling tags...")
    fp = open("/content/-f/tags.pkl","rb")
    tags_arr = pickle.load(fp)
    print("Loading Pickling tags complete")
    fp.close()
    
    #while True:
    for text in res:
  
      rt_test_dataset = PosDataset(construct_input(text),tokenizer,tags_arr[0],tags_arr[1])
      #rt_test_dataset= res
      rt_test_iter = data.DataLoader(dataset=rt_test_dataset,
                              batch_size=8,
                              shuffle=False,
                              num_workers=1,
                              collate_fn=pad)


      ret_arr = test(model, rt_test_iter,tags_arr[0],tags_arr[1])
      predicted_tags.append(ret_arr)
      #print("ret_arr is:",ret_arr)
if __name__== "__main__":
	if (len(sys.argv) < 2):
		print("Specify model dir to load model")
	else:
		test_model(sys.argv[1])

res= [[' '.join(i)] for i in sentence_in]

"""
Calculating Accuracy for the Dev dataset using predicted tags and actual tags
"""
tags=[]
flat_list = [item for sublist in predicted_tags for item in sublist]
for res in flat_list:
  #print(res)
  tags.append(res[1])
acc= accuracy_score(actual_tag[:131768], tags)
acc

"""We can observe that we got an accuracy of 97.64% for the Dev data of WSJ Penn Treebank Dataset

Getting Test data from WSJ Penn treebank dataset
"""

data_path = "/content/drive/MyDrive/NLP_Project/wsj1.test.original"
#predicted_tags = []
sentence_in_test=[]
actual_tag_test=[]
with open(data_path) as f:
    sentences_test = f.readlines()
    temp_test=[]
    for sen in sentences_test:
      sentenceSplit_test = sen.strip("\n").split("\t")
      if sentenceSplit_test[0]=="":
        sentence_in_test.append(temp_test)
        temp_test=[]
        #print(sentence_in)
      else:
        temp_test.append(sentenceSplit_test[1])
        actual_tag_test.append(sentenceSplit_test[4])

res_test= [' '.join(i) for i in sentence_in_test]

"""Predicting the Tags for test data using the saved model"""

predicted_tags_test=[]
def test_model(model_dir):
    device = get_device() 
    tokenizer = get_tokenizer()

    print("Loading model ...")
    model= torch.load("/content/-f/pytorch_model.bin")
    
    print("Loading model complete")
    print("Loading Pickling tags...")
    fp = open("/content/-f/tags.pkl","rb")
    tags_arr = pickle.load(fp)
    print("Loading Pickling tags complete")
    fp.close()
    
    #while True:
    for text in res_test:
  
      rt_test_dataset = PosDataset(construct_input(text),tokenizer,tags_arr[0],tags_arr[1])
      #rt_test_dataset= res
      rt_test_iter = data.DataLoader(dataset=rt_test_dataset,
                              batch_size=8,
                              shuffle=False,
                              num_workers=1,
                              collate_fn=pad)


      ret_arr_test = test(model, rt_test_iter,tags_arr[0],tags_arr[1])
      predicted_tags_test.append(ret_arr_test)
      #print("ret_arr is:",ret_arr)
if __name__== "__main__":
	if (len(sys.argv) < 2):
		print("Specify model dir to load model")
	else:
		test_model(sys.argv[1])

res_test= [[' '.join(i)] for i in sentence_in_test]

"""Calculating the Accuracy score for Test data using actual_tag_test(Actual tags for test data) and tags_test(Predicted tags for test data)"""

"""
Calculating the Accuracy score for Test data using actual_tag_test(Actual tags for test data) and tags_test(Predicted tags for test data)

"""
tags_test=[]
flat_list_test = [item for sublist in predicted_tags_test for item in sublist]
for res in flat_list_test:
  #print(res)
  tags_test.append(res[1])
acc= accuracy_score(actual_tag_test[:129654], tags_test)
acc

len(tags_test)

"""
Printing the list of predicted tags
"""
tags_test

"""
Calculating Accuracy for the test data using tags_test(predicted_tags) list and actual_tag_test(actual tags)
"""
count=0
for i in range(len(tags_test)):
  if tags_test[i]==actual_tag_test[i]:
    count+=1
check=count/len(tags_test)

"""
Converting test data of WSJ Penn tree bank  into CSV
"""
test_2 = csv.reader(open('/content/drive/MyDrive/NLP_Project/wsj1.test.original') , delimiter="\t")
data_test_2=list(test_2)

data_test_2[0]

len(data_test_2)

"""
Removing empty lists from the data_test
"""
res = [ele for ele in data_test_2 if ele != []]

len(res)

len(tags_test)

"""Appending Predicted tags to the Data list"""

for i in range(len(res)):
  res[i].append(tags_test[i])

"""Getting the Predictions for Test data to evaluate the performance of the Model Accuracy"""

"""
Getting the predicted output file for test file 
"""
pred = open('/content/drive/MyDrive/NLP_Project/pred.out', 'w')

for i in range(len(res)):



   pred.write(str(res[i][0])  + '\t' + str(res[i][1]) + '\t' +str(res[i][8])+'\n')
   
pred.close()

